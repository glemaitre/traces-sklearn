{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8340ba",
   "metadata": {},
   "source": [
    "\n",
    "# Regularization\n",
    "\n",
    "This notebook explores regularization in linear models.\n",
    "\n",
    "## Introductory example\n",
    "\n",
    "We demonstrate a common issue with correlated features when fitting linear models.\n",
    "\n",
    "We use the penguins dataset to illustrate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9b9d2",
   "metadata": {},
   "source": [
    "\n",
    "We select features to predict penguin body mass. We remove rows with missing target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Island\",\n",
    "    \"Clutch Completion\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Culmen Length (mm)\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Species\",\n",
    "    \"Sex\",\n",
    "]\n",
    "target = \"Body Mass (g)\"\n",
    "data, target = penguins[features], penguins[target]\n",
    "target = target.dropna()\n",
    "data = data.loc[target.index]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c3082",
   "metadata": {},
   "source": [
    "\n",
    "Let's evaluate a simple linear model using skrub's preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64142161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "model = skrub.tabular_learner(estimator=LinearRegression())\n",
    "model.set_output(transform=\"pandas\")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv_results)[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365bc51",
   "metadata": {},
   "source": [
    "\n",
    "The test score looks good overall but performs poorly on one fold.\n",
    "Let's examine the coefficient values to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [est[-1].coef_ for est in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=cv_results[\"estimator\"][0][-1].feature_names_in_)\n",
    "coefs.plot.box(whis=[0, 100], vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822a027",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "What do you observe? What causes this behavior?\n",
    "Apply the preprocessing chain and check skrub's statistics on the resulting\n",
    "data to understand these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ba43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797483f4",
   "metadata": {},
   "source": [
    "\n",
    "## Ridge regressor - L2 regularization\n",
    "\n",
    "We saw that coefficients can grow arbitrarily large when features correlate.\n",
    "\n",
    "$$\n",
    "loss = (y - X \\beta)^2 + \\alpha \\|\\beta\\|_2\n",
    "$$\n",
    "\n",
    "L2 regularization forces weights toward zero. The parameter $\\alpha$ controls\n",
    "this shrinkage. Scikit-learn implements this as the Ridge model. Let's fit it\n",
    "and examine its effect on weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f407ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = skrub.tabular_learner(estimator=Ridge(alpha=1)).set_output(transform=\"pandas\")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv_results)[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf484439",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [est[-1].coef_ for est in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=cv_results[\"estimator\"][0][-1].feature_names_in_)\n",
    "coefs.plot.box(whis=[0, 100], vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc81647",
   "metadata": {},
   "source": [
    "\n",
    "A small regularization solves the weight problem. We recover the original\n",
    "relationship:\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Try different $\\alpha$ values and examine how they affect the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2133859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f967c",
   "metadata": {},
   "source": [
    "\n",
    "## Lasso regressor - L1 regularization\n",
    "\n",
    "L1 provides another regularization type. It follows this formula:\n",
    "\n",
    "$$\n",
    "loss = (y - X \\beta)^2 + \\alpha \\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "Scikit-learn implements this as the Lasso regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021323b",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Repeat the previous experiment with different $\\alpha$ values and examine how they\n",
    "affect the weights $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75faa9",
   "metadata": {},
   "source": [
    "\n",
    "## Elastic net - Combining L2 and L1 regularization\n",
    "\n",
    "Combining L2 and L1 regularization offers unique benefits: it identifies important\n",
    "features while preventing non-zero coefficients from growing too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = skrub.tabular_learner(estimator=ElasticNet(alpha=10, l1_ratio=0.95))\n",
    "model.set_output(transform=\"pandas\")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv_results)[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69501e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [est[-1].coef_ for est in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=cv_results[\"estimator\"][0][-1].feature_names_in_)\n",
    "coefs.plot.box(whis=[0, 100], vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1385c8",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "How do we choose the regularization parameter? The validation curve helps analyze\n",
    "single parameter effects. It plots scores versus parameter values.\n",
    "\n",
    "Let's use ValidationCurveDisplay to analyze how the alpha parameter affects\n",
    "Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475425e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skrub.tabular_learner(estimator=Ridge()).set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dadb5",
   "metadata": {},
   "source": [
    "\n",
    "We need to find the parameter name for alpha in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ValidationCurveDisplay\n",
    "\n",
    "disp = ValidationCurveDisplay.from_estimator(\n",
    "    model,\n",
    "    data,\n",
    "    target,\n",
    "    cv=cv,\n",
    "    std_display_style=\"errorbar\",\n",
    "    param_name=\"ridge__alpha\",\n",
    "    param_range=np.logspace(-3, 3, num=20),\n",
    "    n_jobs=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0251b",
   "metadata": {},
   "source": [
    "\n",
    "Too much regularization degrades model performance.\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Try a very small alpha (e.g. `1e-16`) and observe its effect on the\n",
    "validation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63023b1a",
   "metadata": {},
   "source": [
    "\n",
    "In practice, we often use grid or random search instead of validation curves\n",
    "to choose regularization parameters. These methods run internal cross-validation\n",
    "to select the best-performing model. Let's demonstrate random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\"ridge__alpha\": loguniform(1e-3, 1e3)}\n",
    "search = RandomizedSearchCV(model, param_distributions, n_iter=10, cv=cv)\n",
    "search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae6536",
   "metadata": {},
   "source": [
    "\n",
    "This approach enables nested cross-validation. The inner loop selects parameters\n",
    "while the outer loop evaluates model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    search, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv_results)[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb89689",
   "metadata": {},
   "source": [
    "\n",
    "Some scikit-learn models efficiently search hyperparameters internally. Models with\n",
    "\"CV\" in their name, like RidgeCV, automatically find optimal regularization\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "model = skrub.tabular_learner(estimator=RidgeCV(alphas=np.logspace(-3, 3, num=100)))\n",
    "model.set_output(transform=\"pandas\")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv_results)[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ba3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [est[-1].alpha_ for est in cv_results[\"estimator\"]]\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1cb77",
   "metadata": {},
   "source": [
    "\n",
    "## What about classification?\n",
    "\n",
    "Classification handles regularization differently. Instead of creating new estimators,\n",
    "regularization becomes a model parameter. LogisticRegression and LinearSVC offer\n",
    "two main models. Both use penalty and C parameters (C inverts regression's alpha).\n",
    "\n",
    "We'll explore parameter C with LogisticRegression. First, let's load classification\n",
    "data to predict penguin species from culmen measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81347bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "data = data[data[\"Species\"].isin([\"Adelie\", \"Chinstrap\"])]\n",
    "data[\"Species\"] = data[\"Species\"].astype(\"category\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]], data[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    edgecolor=\"black\",\n",
    "    s=50,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb0a2a",
   "metadata": {},
   "source": [
    "\n",
    "**QUESTION**\n",
    "\n",
    "What regularization does LogisticRegression use by default? Check the documentation.\n",
    "\n",
    "Let's fit a model and visualize its decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c71c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    model,\n",
    "    X,\n",
    "    response_method=\"decision_function\",\n",
    "    cmap=plt.cm.RdBu,\n",
    "    plot_method=\"pcolormesh\",\n",
    "    shading=\"auto\",\n",
    ")\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    edgecolor=\"black\",\n",
    "    s=50,\n",
    "    ax=display.ax_,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fe6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model.coef_[0], index=X.columns)\n",
    "coef.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe9efd",
   "metadata": {},
   "source": [
    "\n",
    "This example establishes a baseline for studying parameter C effects.\n",
    "The logistic regression loss function is:\n",
    "\n",
    "$$\n",
    "loss = \\frac{1 - \\rho}{2} w^T w + \\rho \\|w\\|_1 + C \\log ( \\exp (y_i (X \\beta)) + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a34a09",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Fit models with different C values and examine how they affect coefficients\n",
    "and decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92294f8",
   "metadata": {},
   "source": [
    "\n",
    "The loss formula shows C affects the data term (error between true and predicted\n",
    "targets). In regression, alpha affects the weights instead. This explains why C\n",
    "inversely relates to alpha."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
