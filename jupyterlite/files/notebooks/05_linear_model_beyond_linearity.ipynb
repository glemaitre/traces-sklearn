{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4baa88",
   "metadata": {},
   "source": [
    "\n",
    "# Beyond linear separations\n",
    "\n",
    "This notebook shows how preprocessing makes linear models flexible enough to fit data\n",
    "with non-linear relationships between features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc423b21",
   "metadata": {},
   "source": [
    "\n",
    "## Limitation of linear separation\n",
    "\n",
    "We create a complex classification toy dataset where a linear model will likely fail.\n",
    "Let's generate the dataset and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "feature_names = [\"Feature #0\", \"Feature #1\"]\n",
    "target_name = \"class\"\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.13, random_state=42)\n",
    "\n",
    "# Store both data and target in a dataframe to ease plotting\n",
    "moons = pd.DataFrame(\n",
    "    np.concatenate([X, y[:, np.newaxis]], axis=1), columns=feature_names + [target_name]\n",
    ")\n",
    "moons[target_name] = moons[target_name].astype(\"category\")\n",
    "X, y = moons[feature_names], moons[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons.plot.scatter(\n",
    "    x=feature_names[0],\n",
    "    y=feature_names[1],\n",
    "    c=y,\n",
    "    s=50,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffff489",
   "metadata": {},
   "source": [
    "\n",
    "Looking at the dataset, we see that a linear separation cannot effectively\n",
    "discriminate between the classes.\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Fit a `LogisticRegression` model on the dataset.\n",
    "2. Use `sklearn.inspection.DecisionBoundaryDisplay` to draw the decision\n",
    "   boundary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7337614",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Fit a `LogisticRegression` model on the dataset but add a\n",
    "   `sklearn.preprocessing.PolynomialFeatures` transformer.\n",
    "2. Use `sklearn.inspection.DecisionBoundaryDisplay` to draw the decision boundary of\n",
    "   the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e11e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19780e5",
   "metadata": {},
   "source": [
    "\n",
    "## What about SVM?\n",
    "\n",
    "Support Vector Machines (SVM) offer another family of linear algorithms. SVMs use a\n",
    "different training approach than logistic regression. The model finds a hyperplane\n",
    "that maximizes the margin to the closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"svc\", LinearSVC())])\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa354f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(model, X, cmap=plt.cm.viridis)\n",
    "moons.plot.scatter(\n",
    "    x=feature_names[0], y=feature_names[1], c=y, s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c276dd",
   "metadata": {},
   "source": [
    "\n",
    "SVMs become non-linear through the \"kernel trick\". This projects data into a higher\n",
    "dimensional space without explicitly building the kernel, only computing dot products.\n",
    "The `SVC` class enables kernel use. We use a polynomial kernel to create something\n",
    "similar to the previous pipeline with `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"svc\", SVC(kernel=\"poly\", degree=3))])\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(model, X, cmap=plt.cm.viridis)\n",
    "moons.plot.scatter(\n",
    "    x=feature_names[0], y=feature_names[1], c=y, s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c41bc",
   "metadata": {},
   "source": [
    "\n",
    "We can also use other kernel types, like the Radial Basis Function (RBF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"svc\", SVC(kernel=\"rbf\"))])\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431284db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(model, X, cmap=plt.cm.viridis)\n",
    "moons.plot.scatter(\n",
    "    x=feature_names[0], y=feature_names[1], c=y, s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dcadd",
   "metadata": {},
   "source": [
    "\n",
    "Note that SVMs do not scale well with large datasets. Sometimes it works better to\n",
    "approximate the kernel explicitly with a transformer like `Nystroem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e685806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline(\n",
    "    [(\"nystroem\", Nystroem()), (\"logistic_regression\", LogisticRegression())]\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442919b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(model, X, cmap=plt.cm.viridis)\n",
    "moons.plot.scatter(\n",
    "    x=feature_names[0], y=feature_names[1], c=y, s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee4d07",
   "metadata": {},
   "source": [
    "\n",
    "The decision boundary looks similar to an SVM with an RBF kernel. Let's demonstrate\n",
    "the scaling limitations of SVM classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef31ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/adult-census-numeric-all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "X = data.drop(columns=target_name)\n",
    "y = data[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b2be8",
   "metadata": {},
   "source": [
    "\n",
    "The dataset contains almost 50,000 samples - quite large for an SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee9d68",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Split the dataset into training and testing sets.\n",
    "2. Create a model with an RBF kernel SVM. Time how long it takes to fit.\n",
    "3. Repeat with a model using Nystroem kernel approximation and logistic regression.\n",
    "4. Compare the test scores of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
