{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0ebda6",
   "metadata": {},
   "source": [
    "\n",
    "# Gaussian process\n",
    "\n",
    "All models that we encounter up to know provide a point-estimate at the\n",
    "moment of prediction. However, none of the models (apart the\n",
    "`QuantileRegression` provide some confidence interval regarding the provided\n",
    "predictions.\n",
    "\n",
    "A family of model known as Gaussian Process allows to obtain such\n",
    "information. In this notebook, we will present the difference of this model\n",
    "compare to models that we already presented.\n",
    "\n",
    "Let's start by generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c01b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"$x$\", ylabel=\"$f(x)$\", title=\"True generative process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0d4e5",
   "metadata": {},
   "source": [
    "\n",
    "## Example with noise-free target\n",
    "\n",
    "In this first example, we will use the true generative process without\n",
    "adding any noise. For training the Gaussian Process regression, we will only\n",
    "select few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92171100",
   "metadata": {},
   "source": [
    "\n",
    "The advantage of a Gaussian kernel is that we can craft a kernel by hand and compose\n",
    "together some base kernels. Here, we will use a radial basis function (RBF) kernel and\n",
    "a constant parameter to fit the amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476d323",
   "metadata": {},
   "source": [
    "\n",
    "In the previous method that we presented, we had a single model where we\n",
    "usually try to find the optimal parameters that best fit the dataset. In\n",
    "Gaussian Process, the paradigm is different: we deal with a distribution of\n",
    "models. We have some *apriori* defined by the prior distribution of the\n",
    "models. The training set will be combined with this prior to provide us a\n",
    "posterior distribution of models.\n",
    "\n",
    "First, let's have a look at the prior distribution of our Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples = gaussian_process.sample_y(X, n_samples=5)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "for idx, single_prior in enumerate(y_samples.T):\n",
    "    ax.plot(\n",
    "        X.ravel(),\n",
    "        single_prior,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Sampled function #{idx + 1}\",\n",
    "    )\n",
    "\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set(\n",
    "    xlabel=\"$x$\", ylabel=\"$f(x)$\", title=\"Sample from the GP prior distribution\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763a56c",
   "metadata": {},
   "source": [
    "\n",
    "The sample from the prior distribution are just random realisation initially. They are\n",
    "far from fitting our true generative model. However, from all the sample, we can\n",
    "indeed have a distribution of models. In this case, we can plot the mean and the 95%\n",
    "confidence interval.\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2398ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "ax.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.2,\n",
    "    label=r\"95% confidence interval\",\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "\n",
    "for idx, single_prior in enumerate(y_samples.T):\n",
    "    ax.plot(\n",
    "        X.ravel(),\n",
    "        single_prior,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Sampled function #{idx + 1}\",\n",
    "    )\n",
    "\n",
    "\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set(\n",
    "    xlabel=\"$x$\", ylabel=\"$f(x)$\", title=\"GP prediction using only prior distribution\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332b24b",
   "metadata": {},
   "source": [
    "\n",
    "Thus, if we plot the true generative process and the prediction, we are far to be\n",
    "happy about our modelisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381156ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"$x$\")\n",
    "_ = plt.ylabel(\"$f(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3833a96",
   "metadata": {},
   "source": [
    "\n",
    "Now, we fit a Gaussian process on these few training samples to see how they will\n",
    "influence the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8444d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_process.fit(X_train, y_train)\n",
    "gaussian_process.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d686e2a",
   "metadata": {},
   "source": [
    "\n",
    "After fitting our model, we see that the hyperparameters of the kernel have been\n",
    "optimized. Now, we will use our kernel to compute the mean prediction of the full\n",
    "dataset and plot the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression \\non noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e83bba",
   "metadata": {},
   "source": [
    "\n",
    "We see that for a prediction made on a data point close to the one from the training\n",
    "set, the 95% confidence has a small amplitude. Whenever a sample falls far from\n",
    "training data, our model's prediction is less accurate and the model prediction is\n",
    "less precise (higher uncertainty).\n",
    "\n",
    "## Example with noisy targets\n",
    "\n",
    "We can repeat a similar experiment adding an additional noise to the target this time.\n",
    "It will allow seeing the effect of the noise on the fitted model.\n",
    "\n",
    "We add some random Gaussian noise to the target with an arbitrary standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2850031",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = 0.5 + 1.0 * rng.random_sample(y_train.shape)\n",
    "y_train_noisy = y_train + rng.normal(0, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6db08",
   "metadata": {},
   "source": [
    "\n",
    "We create a similar Gaussian process model. In addition to the kernel, this\n",
    "time, we specify the parameter `alpha` which can be interpreted as the\n",
    "variance of a Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ce080",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=dy ** 2, n_restarts_optimizer=9\n",
    ")\n",
    "gaussian_process.fit(X_train, y_train_noisy)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f50001",
   "metadata": {},
   "source": [
    "\n",
    "Let's plot the mean prediction and the uncertainty region as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.errorbar(\n",
    "    X_train,\n",
    "    y_train_noisy,\n",
    "    dy,\n",
    "    linestyle=\"None\",\n",
    "    color=\"tab:blue\",\n",
    "    marker=\".\",\n",
    "    markersize=10,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression \\non a noisy dataset\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
