{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18748e0e",
   "metadata": {},
   "source": [
    "\n",
    "# Bagged trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0beae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9953b",
   "metadata": {},
   "source": [
    "\n",
    "## Bagging estimator\n",
    "\n",
    "We see that increasing the depth of the tree leads to an over-fitted model. We can\n",
    "bypass choosing a specific depth by combining several trees together.\n",
    "\n",
    "Let's start by training several trees on slightly different data. We can generate\n",
    "slightly different datasets by randomly sampling with replacement. In statistics, we\n",
    "call this a bootstrap sample. We will use the iris dataset to create such an\n",
    "ensemble and keep some data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a65a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X[:100], y[:100]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c290855",
   "metadata": {},
   "source": [
    "\n",
    "Before training several decision trees, let's run a single tree. Instead of training\n",
    "this tree on `X_train`, we want to train it on a bootstrap sample. We can use the\n",
    "`np.random.choice` function to sample indices with replacement. We need to create a\n",
    "sample_weight vector and pass it to the `fit` method of the `DecisionTreeClassifier`.\n",
    "We provide the `generate_sample_weight` function to generate the `sample_weight` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def bootstrap_idx(X):\n",
    "    indices = np.random.choice(np.arange(X.shape[0]), size=X.shape[0], replace=True)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_idx(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(bootstrap_idx(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965aee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    indices = bootstrap_idx(X)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70816d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bootstrap, y_train_bootstrap = bootstrap_sample(X_train, y_train)\n",
    "\n",
    "print(f\"Classes distribution in the original data: {Counter(y_train)}\")\n",
    "print(f\"Classes distribution in the bootstrap: {Counter(y_train_bootstrap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d4308",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "**EXERCISE**: Create a bagging classifier\n",
    "\n",
    "A bagging classifier trains several decision tree classifiers, each on a different\n",
    "bootstrap sample.\n",
    "\n",
    "1. Create several `DecisionTreeClassifier` instances and store them in a Python list\n",
    "2. Loop through these trees and `fit` them by generating a bootstrap sample using\n",
    "   the `bootstrap_sample` function\n",
    "3. To predict with this ensemble on new data (testing set), provide the same set\n",
    "   to each tree and call the `predict` method. Aggregate all predictions in a NumPy array\n",
    "4. Once you have the predictions, provide a single prediction by keeping the most\n",
    "   predicted class (majority vote)\n",
    "5. Check the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47fe17",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**: Using scikit-learn\n",
    "\n",
    "After implementing your own bagging classifier, use scikit-learn's `BaggingClassifier`\n",
    "to fit the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afedc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9d855",
   "metadata": {},
   "source": [
    "\n",
    "### Note about the base estimator\n",
    "\n",
    "In the previous section, we used a decision tree as the base estimator in the bagging\n",
    "ensemble. However, this method accepts any kind of base estimator. We will compare two\n",
    "bagging models: one uses decision trees and another uses a linear model with a\n",
    "preprocessing step.\n",
    "\n",
    "Let's first create a synthetic regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92221ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a random number generator to set the randomness\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "n_samples = 30\n",
    "x_min, x_max = -3, 3\n",
    "x = rng.uniform(x_min, x_max, size=n_samples)\n",
    "noise = 4.0 * rng.normal(size=n_samples)\n",
    "y = x**3 - 0.5 * (x + 1) ** 2 + noise\n",
    "y /= y.std()\n",
    "\n",
    "data_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "data_test = pd.DataFrame(np.linspace(x_max, x_min, num=300), columns=[\"Feature\"])\n",
    "target_train = pd.Series(y, name=\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2da898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line in JupyterLite\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\", alpha=0.5)\n",
    "ax.set_title(\"Synthetic regression dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72ca6c",
   "metadata": {},
   "source": [
    "\n",
    "We will first train a `BaggingRegressor` where the base estimators are\n",
    "`DecisionTreeRegressor` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c012170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagged_trees = BaggingRegressor(n_estimators=50, random_state=0)\n",
    "bagged_trees.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4b30a",
   "metadata": {},
   "source": [
    "\n",
    "We can make a plot showing the prediction from each individual tree and the averaged\n",
    "response from the bagging regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a62ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree_idx, tree in enumerate(bagged_trees.estimators_):\n",
    "    label = \"Predictions of individual trees\" if tree_idx == 0 else None\n",
    "    tree_predictions = tree.predict(data_test.to_numpy())\n",
    "    plt.plot(\n",
    "        data_test,\n",
    "        tree_predictions,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.1,\n",
    "        color=\"tab:blue\",\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\", alpha=0.5)\n",
    "\n",
    "bagged_trees_predictions = bagged_trees.predict(data_test)\n",
    "plt.plot(\n",
    "    data_test,\n",
    "    bagged_trees_predictions,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictions of ensemble\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12198e94",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will show that we can use a model other than a decision tree. We will create\n",
    "a model that uses `PolynomialFeatures` to augment features followed by a `Ridge`\n",
    "linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5dffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regressor = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PolynomialFeatures(degree=4),\n",
    "    Ridge(alpha=1e-10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba645c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "bagged_trees = BaggingRegressor(\n",
    "    n_estimators=100, estimator=polynomial_regressor, random_state=0\n",
    ")\n",
    "bagged_trees.fit(data_train, target_train)\n",
    "\n",
    "for tree_idx, tree in enumerate(bagged_trees.estimators_):\n",
    "    label = \"Predictions of individual trees\" if tree_idx == 0 else None\n",
    "    tree_predictions = tree.predict(data_test.to_numpy())\n",
    "    plt.plot(\n",
    "        data_test,\n",
    "        tree_predictions,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.1,\n",
    "        color=\"tab:blue\",\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\", alpha=0.5)\n",
    "\n",
    "bagged_trees_predictions = bagged_trees.predict(data_test)\n",
    "plt.plot(\n",
    "    data_test,\n",
    "    bagged_trees_predictions,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictions of ensemble\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437176a",
   "metadata": {},
   "source": [
    "\n",
    "We observe that both base estimators can model our toy example effectively.\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "### Random forest classifier\n",
    "\n",
    "The random forest classifier is a popular variant of the bagging classifier. In\n",
    "addition to bootstrap sampling, random forest uses a random subset of features to find\n",
    "the best split.\n",
    "\n",
    "**EXERCISE**: Create a random forest classifier\n",
    "\n",
    "Use your previous code that generated several `DecisionTreeClassifier` instances.\n",
    "Check the classifier options and modify the parameters to use only $\\sqrt{F}$ features\n",
    "for splitting, where $F$ represents the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a19a7",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**: Using scikit-learn\n",
    "\n",
    "After implementing your own random forest classifier, use scikit-learn's\n",
    "`RandomForestClassifier` to fit the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fab474",
   "metadata": {},
   "source": [
    "\n",
    "### Random forest regressor\n",
    "\n",
    "**EXERCISE**:\n",
    "\n",
    "1. Load the dataset from `sklearn.datasets.fetch_california_housing`\n",
    "2. Fit a `RandomForestRegressor` with default parameters\n",
    "3. Find the number of features used during training\n",
    "4. Identify the differences between `BaggingRegressor` and `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081e838",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The hyperparameters affecting the training process match those of decision trees.\n",
    "Check the documentation for details. Since we work with a forest of trees, we have\n",
    "an additional parameter `n_estimators`. Let's examine how this parameter affects\n",
    "performance using a validation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ad25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line in JupyterLite\n",
    "%pip install pyodide-http\n",
    "# import pyodide_http\n",
    "# pyodide_http.patch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1366310",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**:\n",
    "\n",
    "1. Compute train and test scores to analyze how the `n_estimators` parameter affects\n",
    "   performance. Define a range of values for this parameter\n",
    "2. Plot the train and test scores with confidence intervals\n",
    "\n",
    "Consider: How does increasing the number of trees affect statistical performance?\n",
    "What trade-offs exist with computational performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75147ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57531fac",
   "metadata": {},
   "source": [
    "\n",
    "You can also tune other parameters that control individual tree overfitting.\n",
    "Sometimes shallow trees suffice. However, random forests typically use deep trees\n",
    "since we want to overfit the learners on bootstrap samples - the ensemble\n",
    "combination mitigates this overfitting. Using shallow (underfitted) trees may\n",
    "lead to an underfitted forest."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
